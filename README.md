# ğŸ§  DevDocs AI  
_Your personal ChatGPT for local documents._

> Chat with your own documents locally using AI.

DevDocs AI is a fully offline, privacy-first **RAG (Retrieval-Augmented Generation)** system that lets you index your documents and ask questions in a ChatGPT-style interface.

It allows you to interact with:
- ğŸ“„ PDF files  
- ğŸ“ TXT and Markdown files  
- ğŸ’» Code files (.py, .js, .ts, etc.)  
- ğŸ“š Multiple documents inside one folder  

Everything runs **locally** on your machine using:
- **Ollama** for embeddings and LLM inference  
- **ChromaDB** for vector storage  
- **FastAPI** for the backend  
- A lightweight **HTML frontend** that looks and behaves like ChatGPT  

No cloud. No data sharing. 100% private.

---

## ğŸš€ Features

- ChatGPT-style UI  
- Local document indexing  
- PDF + text + code support  
- Smart chunking of documents  
- Fast semantic vector search  
- Spinner animation while AI is thinking  
- Sidebar for document indexing  
- Source-aware answers  
- Fully offline & privacy-first design  

---

## ğŸ— Architecture

```
Frontend (HTML UI)
        â†“
FastAPI Backend
        â†“
ChromaDB (Vector Store)
        â†“
Ollama (Embeddings + LLM)
```

This architecture represents a real-world **RAG pipeline** used in production-grade GenAI systems.

---

## ğŸ“¦ Requirements

- Python 3.10+  
- Ollama installed locally  

Pull required models:

```bash
ollama pull nomic-embed-text
ollama pull qwen2.5:3b
```

---

## ğŸ›  Installation

Clone the repository:

```bash
git clone <your-repo-url>
cd backend
```

Create and activate a virtual environment:

```bash
python3 -m venv venv
source venv/bin/activate   # Windows: venv\Scripts\activate
```

Install dependencies:

```bash
pip install -r requirements.txt
```

Start the backend server:

```bash
uvicorn app:app --reload
```

Open the UI:

Just open `index.html` in your browser.

---

## ğŸ§ª Usage

1. Open the UI  
2. Enter your document folder path:

```
/home/yourname/Documents/docs
```

3. Click **Index Now**  
4. Sidebar collapses automatically  
5. Start asking questions in the chat  

---

## ğŸ§  Example Questions

- â€œWhat is this document about?â€  
- â€œExplain the main topicâ€  
- â€œSummarize page 2â€  
- â€œWhat libraries are used in this code?â€  
- â€œWhat is the conclusion?â€  

---

## ğŸ”’ Privacy

All processing happens locally:

- No internet calls  
- No API keys  
- No cloud servers  
- Your documents never leave your machine  

This is true **local-first AI**.

---

## ğŸ“ˆ Why this project is powerful

This project demonstrates:

- Real **RAG architecture**  
- **LLM integration** using Ollama  
- **Vector database design** using ChromaDB  
- Backend engineering with FastAPI  
- UI/UX thinking similar to ChatGPT  
- Debugging and system-level thinking  

This is not a tutorial project.  
This is **production-style GenAI engineering**.

---

## ğŸ§‘â€ğŸ’» Author

**Sourabh Bhimagonda Kagilkar**  
GenAI Developer | Data Analyst | Python Engineer  

---

## â­ Final Note

This is not a toy project.  
This is a **real GenAI system** built from scratch.

If you are learning RAG, embeddings, or local LLM deployment, this project is a perfect reference implementation.
